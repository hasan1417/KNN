{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-1 (KNN Classifier)-Part-A\n",
    "## Handwritten digit recognition\n",
    "In this assignment part-A, we will use the built-in library ``sklearn`` to use KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "`MNIST` is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The training set contains 60,000 examples and the test set contains 10,000 examples. In this assignment we will further split the training set to take out 12,000 examples as a validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that downloads a specified MNIST data file from Yann Le Cun's website\n",
    "def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "    print(\"Downloading %s\" % filename)\n",
    "    urlretrieve(source + filename, filename)\n",
    "\n",
    "# Invokes download() if necessary, then reads in images\n",
    "def load_mnist_images(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1,784)\n",
    "    return data\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the training set\n",
    "train_data = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "train_labels = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "\n",
    "## Load the testing set\n",
    "test_data = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "test_labels = load_mnist_labels('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions:  (60000, 784)\n",
      "Number of training labels:  60000\n",
      "Testing dataset dimensions:  (10000, 784)\n",
      "Number of testing labels:  10000\n"
     ]
    }
   ],
   "source": [
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "{np.uint8(0): np.int64(5923), np.uint8(1): np.int64(6742), np.uint8(2): np.int64(5958), np.uint8(3): np.int64(6131), np.uint8(4): np.int64(5842), np.uint8(5): np.int64(5421), np.uint8(6): np.int64(5918), np.uint8(7): np.int64(6265), np.uint8(8): np.int64(5851), np.uint8(9): np.int64(5949)}\n",
      "Test set distribution:\n",
      "{np.uint8(0): np.int64(980), np.uint8(1): np.int64(1135), np.uint8(2): np.int64(1032), np.uint8(3): np.int64(1010), np.uint8(4): np.int64(982), np.uint8(5): np.int64(892), np.uint8(6): np.int64(958), np.uint8(7): np.int64(1028), np.uint8(8): np.int64(974), np.uint8(9): np.int64(1009)}\n"
     ]
    }
   ],
   "source": [
    "## Compute the number of examples of each digit\n",
    "train_digits, train_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set distribution:\")\n",
    "print(dict(zip(train_digits, train_counts)))\n",
    "\n",
    "test_digits, test_counts = np.unique(test_labels, return_counts=True)\n",
    "print(\"Test set distribution:\")\n",
    "print(dict(zip(test_digits, test_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7\n"
     ]
    }
   ],
   "source": [
    "## Define a function that displays a digit given its vector representation\n",
    "def show_digit(x):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\n",
    "def vis_image(index, dataset=\"train\"):\n",
    "    if(dataset==\"train\"): \n",
    "        show_digit(train_data[index,])\n",
    "        label = train_labels[index]\n",
    "    else:\n",
    "        show_digit(test_data[index,])\n",
    "        label = test_labels[index]\n",
    "    print(\"Label \" + str(label))\n",
    "    return\n",
    "\n",
    "## View the first data point in the training set\n",
    "vis_image(0, \"train\")\n",
    "\n",
    "## Now view the first data point in the test set\n",
    "vis_image(0, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Train data to Train and Validate Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m trainx, valx, trainy, valy \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mtrain_data\u001b[49m, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainx, valx, trainy, valy = train_test_split(train_data, train_labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest neighbor classifier--Brute Force Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built-in KNN classifier to perform handwritten digit classification task. Please keep in mind that any hyper-parameter selection shall be performed on the independent validation set and not on the test set. You need to study the KNeighborsClassifier documentation to understand how to use the api with different parameters. In this set of experiments, you need to select 'brute' for the **algorithm** parameter. Record the error rates on the test set and the cpu time taken for evaluation.\n",
    "\n",
    "**Note:** Here you don't have to implement the KNN classifier but you just need to use it from ``sklearn`` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9741\n",
      "Time Taken: 11.2872 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Initialize the KNN classifier with brute force algorithm, Euclidean distance metric and k=3 (you can change k as needed)\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm='brute', metric='euclidean')\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(trainx, trainy)\n",
    "\n",
    "# Predict the labels for the validation set\n",
    "val_pred = knn.predict(valx)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(valy, val_pred)\n",
    "\n",
    "# Calculate the time taken for evaluation\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the results\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Time Taken: {elapsed_time:.4f} seconds\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Faster nearest neighbor methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. If there are $N$ training points in $\\mathbb{R}^d$, this takes $O(N d)$ time.\n",
    "\n",
    "Fortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. `scikit-learn` has fast implementations of two useful nearest neighbor data structures: the _ball tree_ and the _k-d tree_. Record the error rates on the test set and the cpu time taken for evaluation using these two faster methods.\n",
    "\n",
    "**Note:** You need to select 'ball_tree'or 'kd_tree' for the **algorithm** parameter for ``KNeighborsClassifier`` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: ball_tree\n",
      "Test Accuracy: 0.9705\n",
      "Error Rate: 0.0295\n",
      "Time Taken: 561.9257 seconds\n",
      "\n",
      "Algorithm: kd_tree\n",
      "Test Accuracy: 0.9705\n",
      "Error Rate: 0.0295\n",
      "Time Taken: 716.6293 seconds\n",
      "\n",
      "   Algorithm  Test Accuracy  Error Rate  Time Taken\n",
      "0  ball_tree         0.9705      0.0295  561.925722\n",
      "1    kd_tree         0.9705      0.0295  716.629251\n"
     ]
    }
   ],
   "source": [
    "# You can use KNeighborsClassifier with correct values for 'algorithm'\n",
    "### START CODE HERE ###\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# List to store results for both methods\n",
    "results = []\n",
    "\n",
    "# Combining the training and validation sets for final training\n",
    "final_trainx = np.concatenate((trainx, valx))\n",
    "final_trainy = np.concatenate((trainy, valy))\n",
    "\n",
    "# Define the algorithms to test\n",
    "algorithms = ['ball_tree', 'kd_tree']\n",
    "\n",
    "for algo in algorithms:\n",
    "    # Initialize the KNN classifier with the specific algorithm\n",
    "    knn = KNeighborsClassifier(n_neighbors=1, algorithm=algo, metric='euclidean')\n",
    "    \n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the classifier\n",
    "    knn.fit(final_trainx, final_trainy)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    test_pred = knn.predict(test_data)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "    \n",
    "    # Calculate time taken\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Algorithm': algo,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Error Rate': 1 - test_accuracy,\n",
    "        'Time Taken': elapsed_time\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Algorithm: {algo}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Error Rate: {1 - test_accuracy:.4f}\")\n",
    "    print(f\"Time Taken: {elapsed_time:.4f} seconds\\n\")\n",
    "\n",
    "# Display all results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Record CPU time and accuracy for all the three approaches in different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use brute-force, kd-tree, and ball-tree approaches for the following datasets and record the accuracies and cpu time (for evaluation step only). Looking at the results, can you explain it.\n",
    "\n",
    "Datasets:\n",
    "1. Abalone Data Set (https://archive.ics.uci.edu/ml/datasets/abalone)\n",
    "2. Statlog (Landsat Satellite) Data Set (https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite))\n",
    "\n",
    "**Note:** The datasets are provided as attachement in the assignment as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0069811344146728516\n",
      "0.02992081642150879\n",
      "0.0334169864654541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.genfromtxt('Abalone19.csv', delimiter = ',')\n",
    "data = data[:,1:]\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]\n",
    "y = y.astype(np.int64)\n",
    "trainx, tempx, trainy, tempy = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "valx, testx, valy, testy = train_test_split(tempx, tempy, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "trainx = scaler.fit_transform(trainx)\n",
    "valx = scaler.fit_transform(valx)\n",
    "testx = scaler.fit_transform(testx)\n",
    "\n",
    "algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
    "\n",
    "k_values = [29,31,33,35,37,39,41,43]\n",
    "\n",
    "best_k = 0\n",
    "val_acc = 0\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "    knn.fit(trainx, trainy)\n",
    "    y_pred = knn.predict(valx)\n",
    "    acc = accuracy_score(y_pred,valy)\n",
    "    if acc  > val_acc:\n",
    "        best_k = k\n",
    "        val_acc = acc\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k, algorithm=algorithm)\n",
    "    knn.fit(trainx, trainy)\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(testx)\n",
    "    end_time = time.time()\n",
    "    acc = accuracy_score(y_pred,testy)\n",
    "    # print(acc)\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.genfromtxt('Abalone19.csv', delimiter = ',')\n",
    "data = data[:,1:]\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]\n",
    "y = y.astype(np.int64)\n",
    "trainx, tempx, trainy, tempy = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "valx, testx, valy, testy = train_test_split(tempx, tempy, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# trainx = scaler.fit_transform(trainx)\n",
    "# valx = scaler.fit_transform(valx)\n",
    "# testx = scaler.fit_transform(testx)\n",
    "\n",
    "algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
    "\n",
    "k_values = [29,31,33,35,37,39,41,43]\n",
    "\n",
    "best_k = 0\n",
    "val_acc = 0\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "    knn.fit(trainx, trainy)\n",
    "    y_pred = knn.predict(valx)\n",
    "    acc = accuracy_score(y_pred,valy)\n",
    "    if acc  > val_acc:\n",
    "        best_k = k\n",
    "        val_acc = acc\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k, algorithm=algorithm)\n",
    "    knn.fit(trainx, trainy)\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(testx)\n",
    "    end_time = time.time()\n",
    "    acc = accuracy_score(y_pred,testy)\n",
    "    # print(acc)\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: np.str_('M')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, eval_time\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load and preprocess datasets\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m abalone_X, abalone_y \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabalone19.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming 'Rings' is the last column\u001b[39;00m\n\u001b[0;32m     47\u001b[0m satellite_X, satellite_y \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m186_satimage.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming 'Class' is the last column\u001b[39;00m\n\u001b[0;32m     49\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     50\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbalone\u001b[39m\u001b[38;5;124m\"\u001b[39m, abalone_X, abalone_y),\n\u001b[0;32m     51\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSatellite\u001b[39m\u001b[38;5;124m\"\u001b[39m, satellite_X, satellite_y)\n\u001b[0;32m     52\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m(file_path, target_column_index)\u001b[0m\n\u001b[0;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert X to float\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[0;32m     27\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: np.str_('M')"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def load_and_preprocess_data(file_path, target_column_index):\n",
    "    # Load the data\n",
    "    data = np.genfromtxt(file_path, delimiter=',', dtype=str)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = np.delete(data, target_column_index, axis=1)\n",
    "    y = data[:, target_column_index]\n",
    "    \n",
    "    # Encode categorical variables if necessary\n",
    "    le = LabelEncoder()\n",
    "    for i in range(X.shape[1]):\n",
    "        if X[:, i].dtype == 'object':\n",
    "            X[:, i] = le.fit_transform(X[:, i])\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    # Convert X to float\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def evaluate_knn(X_train, X_test, y_train, y_test, algorithm, k=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm=algorithm)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Measure evaluation time\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, eval_time\n",
    "\n",
    "# Load and preprocess datasets\n",
    "abalone_X, abalone_y = load_and_preprocess_data('abalone19.csv', -1)  # Assuming 'Rings' is the last column\n",
    "satellite_X, satellite_y = load_and_preprocess_data('186_satimage.csv', -1)  # Assuming 'Class' is the last column\n",
    "\n",
    "datasets = [\n",
    "    (\"Abalone\", abalone_X, abalone_y),\n",
    "    (\"Satellite\", satellite_X, satellite_y)\n",
    "]\n",
    "\n",
    "algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
    "results = []\n",
    "\n",
    "# Evaluate each dataset\n",
    "for dataset_name, X, y in datasets:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for algorithm in algorithms:\n",
    "        accuracy, eval_time = evaluate_knn(X_train, X_test, y_train, y_test, algorithm)\n",
    "        results.append((dataset_name, algorithm, accuracy, eval_time))\n",
    "\n",
    "# Display results\n",
    "print(\"Dataset | Algorithm | Accuracy | Evaluation Time\")\n",
    "print(\"-\" * 50)\n",
    "for result in results:\n",
    "    print(f\"{result[0]} | {result[1]} | {result[2]:.4f} | {result[3]:.4f}s\")\n",
    "\n",
    "# Analyze results\n",
    "for dataset in ['Abalone', 'Satellite']:\n",
    "    print(f\"\\nAnalysis for {dataset} dataset:\")\n",
    "    dataset_results = [r for r in results if r[0] == dataset]\n",
    "    fastest_algo = min(dataset_results, key=lambda x: x[3])\n",
    "    most_accurate_algo = max(dataset_results, key=lambda x: x[2])\n",
    "    \n",
    "    print(f\"Fastest algorithm: {fastest_algo[1]} (Time: {fastest_algo[3]:.4f}s)\")\n",
    "    print(f\"Most accurate algorithm: {most_accurate_algo[1]} (Accuracy: {most_accurate_algo[2]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## E1. Extra Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are invited to try some more ideas as extra work like:\n",
    "1. Instead of using the pixels as features, implement your own features (example: what were presented in the slides) and use them for KNN. You need to keep in mind that if you compute features which have different scales then it is important to scale/normalize the features (discussed in the slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_custom_features(images):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        # Reshape the image from 1D (784,) to 2D (28, 28)\n",
    "        img = image.reshape((28, 28))\n",
    "        \n",
    "        # Feature 1: Average intensity\n",
    "        avg_intensity = np.mean(img)\n",
    "        \n",
    "        # Feature 2: Standard deviation of intensity\n",
    "        std_intensity = np.std(img)\n",
    "        \n",
    "        # Feature 3: Horizontal symmetry\n",
    "        horz_symmetry = np.mean(np.abs(img - np.fliplr(img)))\n",
    "        \n",
    "        # Feature 4: Vertical symmetry\n",
    "        vert_symmetry = np.mean(np.abs(img - np.flipud(img)))\n",
    "        \n",
    "        # Feature 5: Aspect ratio\n",
    "        aspect_ratio = float(np.count_nonzero(img.sum(axis=0))) / np.count_nonzero(img.sum(axis=1))\n",
    "        \n",
    "        # Combine features into a single vector\n",
    "        feature_vector = [avg_intensity, std_intensity, horz_symmetry, vert_symmetry, aspect_ratio]\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Example usage with the training data\n",
    "train_custom_features = extract_custom_features(train_data)\n",
    "test_custom_features = extract_custom_features(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_custom_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Scale the custom features\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_custom_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtrain_custom_features\u001b[49m)\n\u001b[0;32m      8\u001b[0m test_custom_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_custom_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_custom_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the custom features\n",
    "train_custom_features_scaled = scaler.fit_transform(train_custom_features)\n",
    "test_custom_features_scaled = scaler.transform(test_custom_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Custom Features: 0.3576\n",
      "Test Accuracy with Custom Features: 0.3609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_custom_features_scaled, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier with a specific algorithm\n",
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_pred = knn.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(f\"Validation Accuracy with Custom Features: {val_accuracy:.4f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred = knn.predict(test_custom_features_scaled)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "print(f\"Test Accuracy with Custom Features: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Advanced Features: 0.9261\n",
      "Test Accuracy with Advanced Features: 0.9260\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def extract_advanced_features(images):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        # Reshape the image from 1D (784,) to 2D (28, 28)\n",
    "        img = image.reshape((28, 28))\n",
    "        \n",
    "        # Feature 1: Zoning Features (7x7 grid)\n",
    "        zones = []\n",
    "        for i in range(0, 28, 7):\n",
    "            for j in range(0, 28, 7):\n",
    "                zone = img[i:i+7, j:j+7]\n",
    "                zone_avg = np.mean(zone)\n",
    "                zones.append(zone_avg)\n",
    "        \n",
    "        # Feature 2: HOG Features\n",
    "        # Check if 'multichannel' parameter is supported\n",
    "        try:\n",
    "            # For newer versions of skimage, use 'multichannel=False'\n",
    "            fd = hog(img, orientations=8, pixels_per_cell=(7, 7),\n",
    "                     cells_per_block=(1, 1), visualize=False, multichannel=False)\n",
    "        except TypeError:\n",
    "            # For older versions, remove 'multichannel' parameter\n",
    "            fd = hog(img, orientations=8, pixels_per_cell=(7, 7),\n",
    "                     cells_per_block=(1, 1), visualize=False)\n",
    "        \n",
    "        # Combine features into a single vector\n",
    "        feature_vector = zones + fd.tolist()\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Extract advanced features\n",
    "train_advanced_features = extract_advanced_features(train_data)\n",
    "test_advanced_features = extract_advanced_features(test_data)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "train_advanced_features_scaled = scaler.fit_transform(train_advanced_features)\n",
    "test_advanced_features_scaled = scaler.transform(test_advanced_features)\n",
    "\n",
    "# Train and evaluate the KNN model\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_advanced_features_scaled, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "val_pred = knn.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(f\"Validation Accuracy with Advanced Features: {val_accuracy:.4f}\")\n",
    "\n",
    "# Test accuracy\n",
    "test_pred = knn.predict(test_advanced_features_scaled)\n",
    "test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "print(f\"Test Accuracy with Advanced Features: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to keep in mind the following points:\n",
    "1. Use numpy arrays and numpy libraries for efficient computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
